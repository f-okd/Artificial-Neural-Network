# Artificial-Neural-Network

This report explores the implementation and modification of a Multi-Layer Perceptron (MLP) neural network using Python. The MLP model is trained using gradient descent with backpropagation to minimize prediction errors. Four different MLP variations are investigated: standard MLP, annealing, weight decay, and momentum. Annealing adjusts the learning rate over time to explore the solution space effectively and prevent local minima. Weight decay introduces a penalty term to the error function, controlling its strength with a regularisation parameter that decays over time. Momentum incorporates the previous iteration's weight and bias differences into updates, helping the model maintain direction. These modifications aim to improve the performance and generalization of the MLP model.
